#!/usr/bin/env python3
"""
fetch_news.py
å„ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ™ãƒ³ãƒ€ãƒ¼ã®å…¬å¼ãƒšãƒ¼ã‚¸å¯¾å¿œ RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã€data/news.json ã‚’æ›´æ–°ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

å¯¾å¿œãƒšãƒ¼ã‚¸:
  Azure : https://azure.microsoft.com/ja-jp/updates/  (RSS: https://www.microsoft.com/releasecommunications/api/v2/azure/rss)
  AWS   : https://aws.amazon.com/jp/new/
  GCP   : https://docs.cloud.google.com/release-notes
  OCI   : https://docs.oracle.com/en-us/iaas/releasenotes/
"""

import json
import os
import re
import urllib.request
from datetime import datetime, timezone, timedelta
import feedparser

# ===== è¨­å®š =====
OUTPUT_PATH = os.path.join(os.path.dirname(__file__), "..", "data", "news.json")
MAX_ITEMS_PER_CLOUD = 20   # 1ã‚¯ãƒ©ã‚¦ãƒ‰ã‚ãŸã‚Šä¿å­˜ä»¶æ•°ï¼ˆUIå´ã§è¡¨ç¤ºä»¶æ•°ã‚’åˆ¶å¾¡ï¼‰
MAX_FETCH_ENTRIES = 100    # RSSã‹ã‚‰å–å¾—ã™ã‚‹æœ€å¤§ã‚¨ãƒ³ãƒˆãƒªæ•°ï¼ˆæ—¥ä»˜é™é †ã‚½ãƒ¼ãƒˆç”¨ï¼‰

# RSS ãƒ•ã‚£ãƒ¼ãƒ‰å®šç¾©ï¼ˆå„ã‚¯ãƒ©ã‚¦ãƒ‰ã®æŒ‡å®šå…¬å¼ãƒšãƒ¼ã‚¸ã«å¯¾å¿œã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ï¼‰
FEEDS = {
    "azure": {
        "name": "Microsoft Azure",
        # https://azure.microsoft.com/ja-jp/updates/ ã®å…¬å¼ãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆRSSãƒœã‚¿ãƒ³ã®ãƒªãƒ³ã‚¯å…ˆï¼‰
        "url": "https://www.microsoft.com/releasecommunications/api/v2/azure/rss",
        "fallback_url": "https://azurecomcdn.azureedge.net/en-us/updates/feed/",
    },
    "aws": {
        "name": "Amazon Web Services",
        # https://aws.amazon.com/jp/new/ ã®å…¬å¼ãƒ•ã‚£ãƒ¼ãƒ‰
        "url": "https://aws.amazon.com/jp/new/feed/",
        "fallback_url": "https://aws.amazon.com/new/feed/",
    },
    "gcp": {
        "name": "Google Cloud Platform",
        # https://docs.cloud.google.com/release-notes ã®å…¬å¼ãƒ•ã‚£ãƒ¼ãƒ‰
        "url": "https://cloud.google.com/feeds/gcp-release-notes.xml",
        "fallback_url": "https://cloudblog.withgoogle.com/products/gcp/rss/",
    },
    "oci": {
        "name": "Oracle Cloud Infrastructure",
        # OCI ã«ã¯ RSS ãƒ•ã‚£ãƒ¼ãƒ‰ãŒãªã„ãŸã‚ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—
        "url": "https://docs.oracle.com/en-us/iaas/releasenotes/",
        "scrape": True,
    },
}

# ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (é †ç•ªãŒå„ªå…ˆé †ä½) â€” è‹±èª + æ—¥æœ¬èª
CATEGORY_RULES = [
    ("ai-tag",       ["ai", "ml", "machine learning", "generative", "llm", "bedrock",
                       "sagemaker", "vertex", "foundry", "openai", "gemini", "gpt",
                       "phi", "llama", "diffusion", "inference", "training", "neural",
                       # æ—¥æœ¬èª
                       "äººå·¥çŸ¥èƒ½", "ç”Ÿæˆai", "æ©Ÿæ¢°å­¦ç¿’", "æ¨è«–", "å­¦ç¿’ãƒ¢ãƒ‡ãƒ«", "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ",
                       "ãƒãƒ£ãƒƒãƒˆ", "è¨€èªãƒ¢ãƒ‡ãƒ«", "ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢", "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"]),
    ("security-tag", ["security", "iam", "identity", "auth", "mfa", "zero trust",
                       "compliance", "encryption", "kms", "vault", "sentinel",
                       "defender", "guard", "waf", "shield", "entra",
                       # æ—¥æœ¬èª
                       "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "èªè¨¼", "æš—å·åŒ–", "ã‚¼ãƒ­ãƒˆãƒ©ã‚¹ãƒˆ", "ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£",
                       "ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹", "æ¨©é™ç®¡ç†", "ä¸æ­£ã‚¢ã‚¯ã‚»ã‚¹", "è„†å¼±æ€§", "è„šå¨è„…è¿«"]),
    ("container-tag",["kubernetes", "container", "eks", "aks", "gke", "oke",
                       "docker", "helm", "fargate", "cloud run", "app service",
                       # æ—¥æœ¬èª
                       "ã‚³ãƒ³ãƒ†ãƒŠ", "ã‚¯ãƒãƒãƒ†ã‚£ã‚¹", "ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸", "ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹"]),
    ("database-tag", ["database", "db", "rds", "aurora", "dynamo", "cosmos", "spanner",
                       "alloydb", "sql", "postgres", "mysql", "redis", "mongodb",
                       "autonomous", "heatwave", "bigtable", "firestore",
                       # æ—¥æœ¬èª
                       "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹", "ãƒ‡ãƒ¼ã‚¿åˆ†æ", "ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢",
                       "ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ãƒ“ãƒƒã‚°ã‚¯ã‚¨ãƒª", "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åˆ†æ"]),
    ("storage-tag",  ["storage", "s3", "blob", "bucket", "gcs", "object storage",
                       "efs", "fsx", "archive", "backup",
                       # æ—¥æœ¬èª
                       "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸", "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", "ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–", "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸",
                       "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸", "ãƒ–ãƒ­ãƒƒã‚¯ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸"]),
    ("network-tag",  ["network", "vpc", "vnet", "subnet", "cdn", "cloudfront",
                       "load balancer", "dns", "route", "direct connect",
                       "expressroute", "vpn", "firewall",
                       # æ—¥æœ¬èª
                       "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«", "ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼",
                       "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é…ä¿¡", "å°‚ç”¨ç·š", "vpnæ¥ç¶š", "ã‚µãƒ–ãƒãƒƒãƒˆ"]),
    ("compute-tag",  ["compute", "ec2", "vm", "virtual machine", "instance",
                       "graviton", "cobalt", "axion", "ampere", "gpu", "tpu",
                       "lambda", "functions", "serverless", "batch",
                       # æ—¥æœ¬èª
                       "ä»’æƒ³ãƒã‚·ãƒ³", "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°", "ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹", "ãƒãƒƒãƒå‡¦ç†",
                       "é«˜æ€§èƒ½ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°", "hpc", "ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹", "gpuã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼"]),
]


def detect_category(title: str, summary: str) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚µãƒãƒªãƒ¼ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®šã™ã‚‹"""
    text = (title + " " + summary).lower()
    for tag, keywords in CATEGORY_RULES:
        if any(kw in text for kw in keywords):
            return tag
    return "compute-tag"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ


def detect_category_label(tag: str) -> str:
    """ã‚¿ã‚°åã‹ã‚‰è¡¨ç¤ºãƒ©ãƒ™ãƒ«ã¸ã®å¤‰æ›"""
    labels = {
        "ai-tag":        "AI / ML",
        "security-tag":  "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
        "container-tag": "ã‚³ãƒ³ãƒ†ãƒŠ",
        "database-tag":  "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
        "storage-tag":   "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸",
        "network-tag":   "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "compute-tag":   "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°",
    }
    return labels.get(tag, "ãã®ä»–")


def parse_date(entry) -> tuple[str, str]:
    """
    ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰æ—¥ä»˜ã‚’è§£æã—ã€
    (display_str: "YYYYå¹´MæœˆDæ—¥", iso_str: "YYYY-MM-DD") ã‚’è¿”ã™
    """
    JST = timezone(timedelta(hours=9))
    ts = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if ts:
        dt = datetime(*ts[:6], tzinfo=timezone.utc).astimezone(JST)
    else:
        dt = datetime.now(JST)
    return f"{dt.year}å¹´{dt.month}æœˆ{dt.day}æ—¥", dt.strftime("%Y-%m-%d")


def clean_text(text: str, max_len: int = 180) -> str:
    """HTML ã‚¿ã‚°ã‚’é™¤å»ã—ã€æŒ‡å®šæ–‡å­—æ•°ã«åˆ‡ã‚Šè©°ã‚ã‚‹"""
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    if len(text) > max_len:
        text = text[:max_len].rsplit(" ", 1)[0] + "â€¦"
    return text


def strip_azure_prefix(title: str) -> str:
    """Azure RSS ã‚¿ã‚¤ãƒˆãƒ«ã® '[Launched] ' '[In preview] ' ãªã©ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»"""
    return re.sub(r"^\[(Launched|In preview|In development|Retired)\]\s*", "", title).strip()


def fetch_feed(cloud_key: str, conf: dict) -> list[dict]:
    """æŒ‡å®šã®ã‚¯ãƒ©ã‚¦ãƒ‰ã® RSS ã‚’å–å¾—ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    items = []
    for url in [conf["url"], conf.get("fallback_url", "")]:
        if not url:
            continue
        try:
            feed = feedparser.parse(url)
            if feed.bozo and not feed.entries:
                continue  # è§£æå¤±æ•—
            all_entries = []
            for entry in feed.entries[:MAX_FETCH_ENTRIES]:
                title   = clean_text(entry.get("title", "(ã‚¿ã‚¤ãƒˆãƒ«ãªã—)"), 120)
                if cloud_key == "azure":
                    title = strip_azure_prefix(title)
                summary = clean_text(entry.get("summary", entry.get("description", "")), 200)
                link    = entry.get("link", "")
                date_display, date_iso = parse_date(entry)
                cat_tag   = detect_category(title, summary)
                cat_label = detect_category_label(cat_tag)

                all_entries.append({
                    "title":      title,
                    "link":       link,
                    "summary":    summary,
                    "date":       date_display,
                    "date_iso":   date_iso,
                    "category":   cat_tag,
                    "cat_label":  cat_label,
                    "tag":        cloud_key.upper(),
                })
            # å…¬é–‹æ—¥é™é †ã‚½ãƒ¼ãƒˆ â†’ æœ€å¤§ä»¶æ•°å–å¾—
            all_entries.sort(key=lambda x: x["date_iso"], reverse=True)
            items = all_entries[:MAX_ITEMS_PER_CLOUD]
            if items:
                print(f"  [{cloud_key}] {len(items)} ä»¶å–å¾— ({url})")
                break
        except Exception as e:
            print(f"  [{cloud_key}] å–å¾—å¤±æ•— ({url}): {e}")
    return items


def fetch_aws_from_web() -> list[dict]:
    """AWS What's New ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦æœ€æ–°ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿”ã™ï¼ˆRSS ãƒ•ã‚£ãƒ¼ãƒ‰ä¸å…·åˆæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    url = "https://aws.amazon.com/jp/about-aws/whats-new/recent/"
    try:
        req = urllib.request.Request(url, headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
        })
        with urllib.request.urlopen(req, timeout=30) as resp:
            html = resp.read().decode("utf-8", errors="replace")
    except Exception as e:
        print(f"  [aws] ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {e}")
        return []

    JST = timezone(timedelta(hours=9))
    items = []
    seen_slugs = set()

    # --- ãƒ‘ãƒ¼ã‚¹æˆ¦ç•¥ 1: <a> ã‚¿ã‚°ã‹ã‚‰ whats-new è¨˜äº‹ãƒªãƒ³ã‚¯ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º ---
    # AWS ãƒšãƒ¼ã‚¸ã® HTML æ§‹é€ : <a href="/about-aws/whats-new/YYYY/MM/slug">ã‚¿ã‚¤ãƒˆãƒ«</a>
    link_pattern = re.compile(
        r'<a\s[^>]*href="((?:https://aws\.amazon\.com)?/(?:jp/)?about-aws/whats-new/(\d{4})/(\d{2})/([^"/?#]+))[^"]*"[^>]*>'
        r'([^<]+)</a>',
        re.IGNORECASE
    )

    for m in link_pattern.finditer(html):
        raw_link = m.group(1)
        year = m.group(2)
        month = m.group(3)
        slug = m.group(4).strip().rstrip("/")
        title = m.group(5).strip()

        if slug in seen_slugs:
            continue
        seen_slugs.add(slug)

        title = clean_text(title, 120)
        if not title or len(title) < 5:
            continue

        # URL ã‚’æ­£è¦åŒ–ï¼ˆæ—¥æœ¬èªãƒšãƒ¼ã‚¸ã«æƒãˆã‚‹ï¼‰
        link = f"https://aws.amazon.com/jp/about-aws/whats-new/{year}/{month}/{slug}/"

        # æ—¥ä»˜: ãƒªãƒ³ã‚¯è¿‘è¾ºã‹ã‚‰ YYYY-MM-DD ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        date_iso = f"{year}-{month}-01"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æœˆåˆ
        date_display = f"{year}å¹´{int(month)}æœˆ1æ—¥"
        link_pos = m.start()
        # ãƒªãƒ³ã‚¯ã®å‰å¾Œ 500 æ–‡å­—ä»¥å†…ã§æ—¥ä»˜ã‚’æ¢ã™
        search_start = max(0, link_pos - 300)
        search_end = min(len(html), m.end() + 500)
        nearby_html = html[search_start:search_end]
        date_match = re.search(r'(\d{4})-(\d{2})-(\d{2})', nearby_html)
        if date_match:
            date_iso = date_match.group(0)
            try:
                y = int(date_match.group(1))
                mo = int(date_match.group(2))
                d = int(date_match.group(3))
                date_display = f"{y}å¹´{mo}æœˆ{d}æ—¥"
            except ValueError:
                pass

        cat_tag = detect_category(title, "")
        cat_label = detect_category_label(cat_tag)
        items.append({
            "title":     title,
            "link":      link,
            "summary":   "",
            "date":      date_display,
            "date_iso":  date_iso,
            "category":  cat_tag,
            "cat_label": cat_label,
            "tag":       "AWS",
        })

    # æ—¥ä»˜é™é †ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ N ä»¶ã‚’è¿”ã™
    items.sort(key=lambda x: x["date_iso"], reverse=True)
    result = items[:MAX_ITEMS_PER_CLOUD]
    if result:
        print(f"  [aws] {len(result)} ä»¶å–å¾— (web scraping: {url})")
    else:
        print(f"  [aws] 0 ä»¶å–å¾— (ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—ã¾ãŸã¯ã‚¢ã‚¤ãƒ†ãƒ ãªã—)")
    return result


def merge_aws_items(rss_items: list[dict], web_items: list[dict]) -> list[dict]:
    """RSS ã¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦é‡è¤‡ã‚’æ’é™¤ã—ã€æœ€æ–°é †ã«ä¸¦ã¹ã‚‹"""
    seen = {}
    for item in rss_items + web_items:
        # URL ã®ã‚¹ãƒ©ãƒƒã‚°ã§é‡è¤‡åˆ¤å®š
        slug = item["link"].rstrip("/").split("/")[-1]
        if slug not in seen or item["date_iso"] > seen[slug]["date_iso"]:
            seen[slug] = item
    merged = list(seen.values())
    merged.sort(key=lambda x: x["date_iso"], reverse=True)
    return merged[:MAX_ITEMS_PER_CLOUD]


def fetch_oci_from_web() -> list[dict]:
    """OCI ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦æœ€æ–°ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿”ã™ï¼ˆRSS å»ƒæ­¢å¯¾å¿œï¼‰"""
    url = "https://docs.oracle.com/en-us/iaas/releasenotes/"
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0 (compatible; cloud-news-fetcher/1.0)"})
        with urllib.request.urlopen(req, timeout=25) as resp:
            html = resp.read().decode("utf-8", errors="replace")
    except Exception as e:
        print(f"  [oci] ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {e}")
        return []

    JST = timezone(timedelta(hours=9))
    items = []

    # h2 ã‚¿ã‚°ã§åŒºåˆ‡ã‚Šã€å„ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ãƒªãƒ³ã‚¯ãƒ»æ—¥ä»˜ã‚’æŠ½å‡º
    blocks = re.split(r'<h2[^>]*>', html, flags=re.IGNORECASE)[1:]
    for block in blocks[:60]:
        # æœ€åˆã® <a href> ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªãƒ³ã‚¯ã‚’å–å¾—
        link_match = re.search(r'<a\s+href="([^"]+)"[^>]*>([^<]+)</a>', block)
        if not link_match:
            continue
        link = link_match.group(1).strip()
        title = clean_text(link_match.group(2))
        if not title or len(title) < 5 or 'ğŸ”—' in title:
            continue
        # ç›¸å¯¾ URL ã‚’çµ¶å¯¾ URL ã«å¤‰æ›
        if link.startswith("/"):
            link = "https://docs.oracle.com" + link
        elif not link.startswith("http"):
            continue

        # ãƒ–ãƒ­ãƒƒã‚¯å†…ã‹ã‚‰ Release Date ã‚’æŠ½å‡º
        # HTMLæ§‹é€ : <span class="vl-relnotedate">February 20, 2026</span>
        date_match = re.search(r'vl-relnotedate">([A-Za-z]+ \d+, \d{4})', block)
        if not date_match:
            continue
        try:
            dt = datetime.strptime(date_match.group(1), "%B %d, %Y")
            date_iso = dt.strftime("%Y-%m-%d")
            date_display = f"{dt.year}å¹´{dt.month}æœˆ{dt.day}æ—¥"
        except ValueError:
            continue

        cat_tag = detect_category(title, "")
        cat_label = detect_category_label(cat_tag)
        items.append({
            "title":     title,
            "link":      link,
            "summary":   "",
            "date":      date_display,
            "date_iso":  date_iso,
            "category":  cat_tag,
            "cat_label": cat_label,
            "tag":       "OCI",
        })

    # æ—¥ä»˜é™é †ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ N ä»¶ã‚’è¿”ã™
    items.sort(key=lambda x: x["date_iso"], reverse=True)
    result = items[:MAX_ITEMS_PER_CLOUD]
    if result:
        print(f"  [oci] {len(result)} ä»¶å–å¾— (web scraping: {url})")
    else:
        print(f"  [oci] 0 ä»¶å–å¾— (ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—ã¾ãŸã¯ã‚¢ã‚¤ãƒ†ãƒ ãªã—)")
    return result


def main():
    JST = timezone(timedelta(hours=9))
    now_str = datetime.now(JST).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M JST")
    news = {
        "updated": now_str,
        "clouds": {}
    }

    for cloud_key, conf in FEEDS.items():
        print(f"Fetching {conf['name']} ...")
        if conf.get("scrape"):
            items = fetch_oci_from_web()
        elif cloud_key == "aws":
            # AWS: RSS ãƒ•ã‚£ãƒ¼ãƒ‰ãŒå¤ã„å ´åˆã«ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            rss_items = fetch_feed(cloud_key, conf)
            # RSS ã®æœ€æ–°ã‚¨ãƒ³ãƒˆãƒªãŒ 7 æ—¥ä»¥ä¸Šå‰ãªã‚‰ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚‚è©¦è¡Œ
            today = datetime.now(JST).strftime("%Y-%m-%d")
            latest_rss = rss_items[0]["date_iso"] if rss_items else "2000-01-01"
            try:
                days_old = (datetime.strptime(today, "%Y-%m-%d") - datetime.strptime(latest_rss, "%Y-%m-%d")).days
            except ValueError:
                days_old = 999
            if days_old > 7:
                print(f"  [aws] RSS ãƒ‡ãƒ¼ã‚¿ãŒ {days_old} æ—¥å‰ã®ãŸã‚ã€ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚‚è©¦è¡Œã—ã¾ã™...")
                web_items = fetch_aws_from_web()
                items = merge_aws_items(rss_items, web_items)
            else:
                items = rss_items
        else:
            items = fetch_feed(cloud_key, conf)
        news["clouds"][cloud_key] = items

    # å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(news, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ data/news.json ã‚’æ›´æ–°ã—ã¾ã—ãŸ ({now_str})")


if __name__ == "__main__":
    main()
