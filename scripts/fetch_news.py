#!/usr/bin/env python3
"""
fetch_news.py
å„ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ™ãƒ³ãƒ€ãƒ¼ã®å…¬å¼ãƒšãƒ¼ã‚¸å¯¾å¿œ RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã€data/news.json ã‚’æ›´æ–°ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

å¯¾å¿œãƒšãƒ¼ã‚¸:
  Azure : https://azure.microsoft.com/ja-jp/updates/  (RSS: https://www.microsoft.com/releasecommunications/api/v2/azure/rss)
  AWS   : https://aws.amazon.com/jp/new/
  GCP   : https://docs.cloud.google.com/release-notes
  OCI   : https://docs.oracle.com/en-us/iaas/releasenotes/
"""

import json
import os
import re
import urllib.request
from datetime import datetime, timezone, timedelta
import feedparser

# ===== è¨­å®š =====
OUTPUT_PATH = os.path.join(os.path.dirname(__file__), "..", "data", "news.json")
MAX_ITEMS_PER_CLOUD = 20   # 1ã‚¯ãƒ©ã‚¦ãƒ‰ã‚ãŸã‚Šä¿å­˜ä»¶æ•°ï¼ˆUIå´ã§è¡¨ç¤ºä»¶æ•°ã‚’åˆ¶å¾¡ï¼‰
MAX_FETCH_ENTRIES = 100    # RSSã‹ã‚‰å–å¾—ã™ã‚‹æœ€å¤§ã‚¨ãƒ³ãƒˆãƒªæ•°ï¼ˆæ—¥ä»˜é™é †ã‚½ãƒ¼ãƒˆç”¨ï¼‰

# RSS ãƒ•ã‚£ãƒ¼ãƒ‰å®šç¾©ï¼ˆå„ã‚¯ãƒ©ã‚¦ãƒ‰ã®æŒ‡å®šå…¬å¼ãƒšãƒ¼ã‚¸ã«å¯¾å¿œã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ï¼‰
FEEDS = {
    "azure": {
        "name": "Microsoft Azure",
        # https://azure.microsoft.com/ja-jp/updates/ ã®å…¬å¼ãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆRSSãƒœã‚¿ãƒ³ã®ãƒªãƒ³ã‚¯å…ˆï¼‰
        "url": "https://www.microsoft.com/releasecommunications/api/v2/azure/rss",
        "fallback_url": "https://azurecomcdn.azureedge.net/en-us/updates/feed/",
    },
    "aws": {
        "name": "Amazon Web Services",
        # https://aws.amazon.com/jp/new/ ã®å…¬å¼ãƒ•ã‚£ãƒ¼ãƒ‰
        "url": "https://aws.amazon.com/jp/new/feed/",
        "fallback_url": "https://aws.amazon.com/new/feed/",
    },
    "gcp": {
        "name": "Google Cloud Platform",
        # https://docs.cloud.google.com/release-notes ã®å…¬å¼ãƒ•ã‚£ãƒ¼ãƒ‰
        "url": "https://cloud.google.com/feeds/gcp-release-notes.xml",
        "fallback_url": "https://cloudblog.withgoogle.com/products/gcp/rss/",
    },
    "oci": {
        "name": "Oracle Cloud Infrastructure",
        # OCI ã«ã¯ RSS ãƒ•ã‚£ãƒ¼ãƒ‰ãŒãªã„ãŸã‚ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—
        "url": "https://docs.oracle.com/en-us/iaas/releasenotes/",
        "scrape": True,
    },
}

# ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (é †ç•ªãŒå„ªå…ˆé †ä½) â€” è‹±èª + æ—¥æœ¬èª
CATEGORY_RULES = [
    ("ai-tag",       ["ai", "ml", "machine learning", "generative", "llm", "bedrock",
                       "sagemaker", "vertex", "foundry", "openai", "gemini", "gpt",
                       "phi", "llama", "diffusion", "inference", "training", "neural",
                       # æ—¥æœ¬èª
                       "äººå·¥çŸ¥èƒ½", "ç”Ÿæˆai", "æ©Ÿæ¢°å­¦ç¿’", "æ¨è«–", "å­¦ç¿’ãƒ¢ãƒ‡ãƒ«", "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ",
                       "ãƒãƒ£ãƒƒãƒˆ", "è¨€èªãƒ¢ãƒ‡ãƒ«", "ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢", "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"]),
    ("security-tag", ["security", "iam", "identity", "auth", "mfa", "zero trust",
                       "compliance", "encryption", "kms", "vault", "sentinel",
                       "defender", "guard", "waf", "shield", "entra",
                       # æ—¥æœ¬èª
                       "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "èªè¨¼", "æš—å·åŒ–", "ã‚¼ãƒ­ãƒˆãƒ©ã‚¹ãƒˆ", "ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£",
                       "ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹", "æ¨©é™ç®¡ç†", "ä¸æ­£ã‚¢ã‚¯ã‚»ã‚¹", "è„†å¼±æ€§", "è„šå¨è„…è¿«"]),
    ("container-tag",["kubernetes", "container", "eks", "aks", "gke", "oke",
                       "docker", "helm", "fargate", "cloud run", "app service",
                       # æ—¥æœ¬èª
                       "ã‚³ãƒ³ãƒ†ãƒŠ", "ã‚¯ãƒãƒãƒ†ã‚£ã‚¹", "ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸", "ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹"]),
    ("database-tag", ["database", "db", "rds", "aurora", "dynamo", "cosmos", "spanner",
                       "alloydb", "sql", "postgres", "mysql", "redis", "mongodb",
                       "autonomous", "heatwave", "bigtable", "firestore",
                       # æ—¥æœ¬èª
                       "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹", "ãƒ‡ãƒ¼ã‚¿åˆ†æ", "ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢",
                       "ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ãƒ“ãƒƒã‚°ã‚¯ã‚¨ãƒª", "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åˆ†æ"]),
    ("storage-tag",  ["storage", "s3", "blob", "bucket", "gcs", "object storage",
                       "efs", "fsx", "archive", "backup",
                       # æ—¥æœ¬èª
                       "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸", "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", "ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–", "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸",
                       "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸", "ãƒ–ãƒ­ãƒƒã‚¯ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸"]),
    ("network-tag",  ["network", "vpc", "vnet", "subnet", "cdn", "cloudfront",
                       "load balancer", "dns", "route", "direct connect",
                       "expressroute", "vpn", "firewall",
                       # æ—¥æœ¬èª
                       "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«", "ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼",
                       "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é…ä¿¡", "å°‚ç”¨ç·š", "vpnæ¥ç¶š", "ã‚µãƒ–ãƒãƒƒãƒˆ"]),
    ("compute-tag",  ["compute", "ec2", "vm", "virtual machine", "instance",
                       "graviton", "cobalt", "axion", "ampere", "gpu", "tpu",
                       "lambda", "functions", "serverless", "batch",
                       # æ—¥æœ¬èª
                       "ä»’æƒ³ãƒã‚·ãƒ³", "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°", "ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹", "ãƒãƒƒãƒå‡¦ç†",
                       "é«˜æ€§èƒ½ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°", "hpc", "ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹", "gpuã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼"]),
]


def detect_category(title: str, summary: str) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚µãƒãƒªãƒ¼ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®šã™ã‚‹"""
    text = (title + " " + summary).lower()
    for tag, keywords in CATEGORY_RULES:
        if any(kw in text for kw in keywords):
            return tag
    return "compute-tag"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ


def detect_category_label(tag: str) -> str:
    """ã‚¿ã‚°åã‹ã‚‰è¡¨ç¤ºãƒ©ãƒ™ãƒ«ã¸ã®å¤‰æ›"""
    labels = {
        "ai-tag":        "AI / ML",
        "security-tag":  "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
        "container-tag": "ã‚³ãƒ³ãƒ†ãƒŠ",
        "database-tag":  "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
        "storage-tag":   "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸",
        "network-tag":   "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "compute-tag":   "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°",
    }
    return labels.get(tag, "ãã®ä»–")


def parse_date(entry) -> tuple[str, str]:
    """
    ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰æ—¥ä»˜ã‚’è§£æã—ã€
    (display_str: "YYYYå¹´MæœˆDæ—¥", iso_str: "YYYY-MM-DD") ã‚’è¿”ã™
    """
    JST = timezone(timedelta(hours=9))
    ts = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if ts:
        dt = datetime(*ts[:6], tzinfo=timezone.utc).astimezone(JST)
    else:
        dt = datetime.now(JST)
    return f"{dt.year}å¹´{dt.month}æœˆ{dt.day}æ—¥", dt.strftime("%Y-%m-%d")


def clean_text(text: str, max_len: int = 180) -> str:
    """HTML ã‚¿ã‚°ã‚’é™¤å»ã—ã€æŒ‡å®šæ–‡å­—æ•°ã«åˆ‡ã‚Šè©°ã‚ã‚‹"""
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    if len(text) > max_len:
        text = text[:max_len].rsplit(" ", 1)[0] + "â€¦"
    return text


def strip_azure_prefix(title: str) -> str:
    """Azure RSS ã‚¿ã‚¤ãƒˆãƒ«ã® '[Launched] ' '[In preview] ' ãªã©ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»"""
    return re.sub(r"^\[(Launched|In preview|In development|Retired)\]\s*", "", title).strip()


def fetch_feed(cloud_key: str, conf: dict) -> list[dict]:
    """æŒ‡å®šã®ã‚¯ãƒ©ã‚¦ãƒ‰ã® RSS ã‚’å–å¾—ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    items = []
    for url in [conf["url"], conf.get("fallback_url", "")]:
        if not url:
            continue
        try:
            feed = feedparser.parse(url)
            if feed.bozo and not feed.entries:
                continue  # è§£æå¤±æ•—
            all_entries = []
            for entry in feed.entries[:MAX_FETCH_ENTRIES]:
                title   = clean_text(entry.get("title", "(ã‚¿ã‚¤ãƒˆãƒ«ãªã—)"), 120)
                if cloud_key == "azure":
                    title = strip_azure_prefix(title)
                summary = clean_text(entry.get("summary", entry.get("description", "")), 200)
                link    = entry.get("link", "")
                date_display, date_iso = parse_date(entry)
                cat_tag   = detect_category(title, summary)
                cat_label = detect_category_label(cat_tag)

                all_entries.append({
                    "title":      title,
                    "link":       link,
                    "summary":    summary,
                    "date":       date_display,
                    "date_iso":   date_iso,
                    "category":   cat_tag,
                    "cat_label":  cat_label,
                    "tag":        cloud_key.upper(),
                })
            # å…¬é–‹æ—¥é™é †ã‚½ãƒ¼ãƒˆ â†’ æœ€å¤§ä»¶æ•°å–å¾—
            all_entries.sort(key=lambda x: x["date_iso"], reverse=True)
            items = all_entries[:MAX_ITEMS_PER_CLOUD]
            if items:
                print(f"  [{cloud_key}] {len(items)} ä»¶å–å¾— ({url})")
                break
        except Exception as e:
            print(f"  [{cloud_key}] å–å¾—å¤±æ•— ({url}): {e}")
    return items


def fetch_aws_from_api() -> list[dict]:
    """AWS What's New v2 Directory API ã‹ã‚‰æœ€æ–°ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—ã™ã‚‹ï¼ˆRSS ãƒ•ã‚£ãƒ¼ãƒ‰ä¸å…·åˆæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    api_url = (
        "https://aws.amazon.com/api/dirs/items/search"
        "?item.directoryId=whats-new-v2"
        "&sort_by=item.additionalFields.postDateTime"
        "&sort_order=desc"
        f"&size={MAX_ITEMS_PER_CLOUD}"
        "&item.locale=ja_JP"
    )
    try:
        req = urllib.request.Request(api_url, headers={
            "User-Agent": "Mozilla/5.0 (compatible; cloud-news-fetcher/1.0)",
            "Accept": "application/json",
        })
        with urllib.request.urlopen(req, timeout=30) as resp:
            data = json.loads(resp.read().decode("utf-8", errors="replace"))
    except Exception as e:
        print(f"  [aws] API å–å¾—å¤±æ•—: {e}")
        return []

    JST = timezone(timedelta(hours=9))
    items = []

    for entry in data.get("items", []):
        fields = entry.get("item", {}).get("additionalFields", {})
        headline = fields.get("headline", "").strip()
        headline_url = fields.get("headlineUrl", "").strip()
        post_dt_str = fields.get("postDateTime", "")
        summary_html = fields.get("postSummary", "")

        if not headline or not headline_url:
            continue

        title = clean_text(headline, 120)

        # URL ã‚’æ­£è¦åŒ–ï¼ˆæ—¥æœ¬èªãƒšãƒ¼ã‚¸ã®çµ¶å¯¾ URL ã«æƒãˆã‚‹ï¼‰
        if headline_url.startswith("/"):
            link = f"https://aws.amazon.com/jp{headline_url}"
        elif headline_url.startswith("http"):
            link = headline_url
        else:
            link = f"https://aws.amazon.com/jp/about-aws/whats-new/{headline_url}"

        # æ—¥ä»˜è§£æ (ISO å½¢å¼: "2026-02-06T18:00:00Z")
        date_iso = "2000-01-01"
        date_display = ""
        if post_dt_str:
            try:
                dt = datetime.strptime(post_dt_str[:19], "%Y-%m-%dT%H:%M:%S")
                dt = dt.replace(tzinfo=timezone.utc).astimezone(JST)
                date_iso = dt.strftime("%Y-%m-%d")
                date_display = f"{dt.year}å¹´{dt.month}æœˆ{dt.day}æ—¥"
            except ValueError:
                date_iso = post_dt_str[:10] if len(post_dt_str) >= 10 else "2000-01-01"
                date_display = date_iso

        summary = clean_text(summary_html, 200) if summary_html else ""

        cat_tag = detect_category(title, summary)
        cat_label = detect_category_label(cat_tag)
        items.append({
            "title":     title,
            "link":      link,
            "summary":   summary,
            "date":      date_display,
            "date_iso":  date_iso,
            "category":  cat_tag,
            "cat_label": cat_label,
            "tag":       "AWS",
        })

    # æ—¥ä»˜é™é †ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ N ä»¶ã‚’è¿”ã™
    items.sort(key=lambda x: x["date_iso"], reverse=True)
    result = items[:MAX_ITEMS_PER_CLOUD]
    if result:
        print(f"  [aws] {len(result)} ä»¶å–å¾— (whats-new-v2 API)")
    else:
        print(f"  [aws] 0 ä»¶å–å¾— (API ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ å–å¾—å¤±æ•—)")
    return result


def merge_aws_items(rss_items: list[dict], web_items: list[dict]) -> list[dict]:
    """RSS ã¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦é‡è¤‡ã‚’æ’é™¤ã—ã€æœ€æ–°é †ã«ä¸¦ã¹ã‚‹"""
    seen = {}
    for item in rss_items + web_items:
        # URL ã®ã‚¹ãƒ©ãƒƒã‚°ã§é‡è¤‡åˆ¤å®š
        slug = item["link"].rstrip("/").split("/")[-1]
        if slug not in seen or item["date_iso"] > seen[slug]["date_iso"]:
            seen[slug] = item
    merged = list(seen.values())
    merged.sort(key=lambda x: x["date_iso"], reverse=True)
    return merged[:MAX_ITEMS_PER_CLOUD]


def fetch_oci_from_web() -> list[dict]:
    """OCI ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦æœ€æ–°ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿”ã™ï¼ˆRSS å»ƒæ­¢å¯¾å¿œï¼‰"""
    url = "https://docs.oracle.com/en-us/iaas/releasenotes/"
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0 (compatible; cloud-news-fetcher/1.0)"})
        with urllib.request.urlopen(req, timeout=25) as resp:
            html = resp.read().decode("utf-8", errors="replace")
    except Exception as e:
        print(f"  [oci] ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {e}")
        return []

    JST = timezone(timedelta(hours=9))
    items = []

    # h2 ã‚¿ã‚°ã§åŒºåˆ‡ã‚Šã€å„ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ãƒªãƒ³ã‚¯ãƒ»æ—¥ä»˜ã‚’æŠ½å‡º
    blocks = re.split(r'<h2[^>]*>', html, flags=re.IGNORECASE)[1:]
    for block in blocks[:60]:
        # æœ€åˆã® <a href> ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªãƒ³ã‚¯ã‚’å–å¾—
        link_match = re.search(r'<a\s+href="([^"]+)"[^>]*>([^<]+)</a>', block)
        if not link_match:
            continue
        link = link_match.group(1).strip()
        title = clean_text(link_match.group(2))
        if not title or len(title) < 5 or 'ğŸ”—' in title:
            continue
        # ç›¸å¯¾ URL ã‚’çµ¶å¯¾ URL ã«å¤‰æ›
        if link.startswith("/"):
            link = "https://docs.oracle.com" + link
        elif not link.startswith("http"):
            continue

        # ãƒ–ãƒ­ãƒƒã‚¯å†…ã‹ã‚‰ Release Date ã‚’æŠ½å‡º
        # HTMLæ§‹é€ : <span class="vl-relnotedate">February 20, 2026</span>
        date_match = re.search(r'vl-relnotedate">([A-Za-z]+ \d+, \d{4})', block)
        if not date_match:
            continue
        try:
            dt = datetime.strptime(date_match.group(1), "%B %d, %Y")
            date_iso = dt.strftime("%Y-%m-%d")
            date_display = f"{dt.year}å¹´{dt.month}æœˆ{dt.day}æ—¥"
        except ValueError:
            continue

        cat_tag = detect_category(title, "")
        cat_label = detect_category_label(cat_tag)
        items.append({
            "title":     title,
            "link":      link,
            "summary":   "",
            "date":      date_display,
            "date_iso":  date_iso,
            "category":  cat_tag,
            "cat_label": cat_label,
            "tag":       "OCI",
        })

    # æ—¥ä»˜é™é †ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ N ä»¶ã‚’è¿”ã™
    items.sort(key=lambda x: x["date_iso"], reverse=True)
    result = items[:MAX_ITEMS_PER_CLOUD]
    if result:
        print(f"  [oci] {len(result)} ä»¶å–å¾— (web scraping: {url})")
    else:
        print(f"  [oci] 0 ä»¶å–å¾— (ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—ã¾ãŸã¯ã‚¢ã‚¤ãƒ†ãƒ ãªã—)")
    return result


def main():
    JST = timezone(timedelta(hours=9))
    now_str = datetime.now(JST).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M JST")
    news = {
        "updated": now_str,
        "clouds": {}
    }

    for cloud_key, conf in FEEDS.items():
        print(f"Fetching {conf['name']} ...")
        if conf.get("scrape"):
            items = fetch_oci_from_web()
        elif cloud_key == "aws":
            # AWS: ã¾ãš whats-new-v2 API ã‚’è©¦è¡Œã—ã€å¤±æ•—æ™‚ã¯ RSS ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            api_items = fetch_aws_from_api()
            if api_items:
                # API æˆåŠŸ â€” RSS ã‚‚å–å¾—ã—ã¦ãƒãƒ¼ã‚¸ï¼ˆAPI ã«ãªã„å¤ã„è¨˜äº‹ã‚’è£œå®Œï¼‰
                rss_items = fetch_feed(cloud_key, conf)
                items = merge_aws_items(api_items, rss_items)
            else:
                # API å¤±æ•—æ™‚ã¯ RSS ã®ã¿
                print(f"  [aws] API å–å¾—å¤±æ•—ã®ãŸã‚ RSS ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                items = fetch_feed(cloud_key, conf)
        else:
            items = fetch_feed(cloud_key, conf)
        news["clouds"][cloud_key] = items

    # å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(news, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ data/news.json ã‚’æ›´æ–°ã—ã¾ã—ãŸ ({now_str})")


if __name__ == "__main__":
    main()
